# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.

""" Storage writer for PyT Distributed format allowing asynchronous save. """  # trace_info : t_31452

import logging                                                                 # trace_info : t_31453
import os                                                                      # trace_info : t_31454
from itertools import chain                                                    # trace_info : t_31455
from pathlib import Path                                                       # trace_info : t_31456
from time import time                                                          # trace_info : t_31457
from typing import Callable, Dict, List, Optional, Tuple                       # trace_info : t_31458

import psutil                                                                  # trace_info : t_31459
import torch                                                                   # trace_info : t_31460
from torch import multiprocessing as mp                                        # trace_info : t_31461
from torch.distributed.checkpoint import FileSystemWriter                      # trace_info : t_31462
from torch.distributed.checkpoint.filesystem import DEFAULT_SUFFIX, _StoragePrefix, _write_item# trace_info : t_31463
from torch.distributed.checkpoint.planner import SavePlan, SavePlanner, WriteItem, WriteItemType# trace_info : t_31464
from torch.distributed.checkpoint.storage import WriteResult                   # trace_info : t_31465
from torch.futures import Future                                               # trace_info : t_31466

logger = logging.getLogger(__name__)                                           # trace_info : t_31467

WriteBucket = Tuple[Path, str, Tuple[list, list]]  # represents writes to a single file# trace_info : t_31468


class FileSystemWriterAsync(FileSystemWriter):                                 # trace_info : t_31469, t_31470
    """                                                                        # trace_info : t_31471
    Async-enabled implementation of FileSystemWriter using file IO.

    This class doesn't spawn the async process itself, relies on the external async mechanism.

    Flow:
    1. Call `write_data`
    2. Externally start async process with `get_save_function_and_args` function and args
    3. The async function to call is `writer_proxy_func` which calls
       `write_preloaded_data` in multiple processes

    After saving is finalized on all ranks:
    4. Call `super().finish` with the results gathered in `self.writer_result`

    Note that step (3) above can also be called synchronously.

    Currently, it's assumed that a separate writer is created for each ckpt save
    (intermediate state is stored as writer attributes).
    """

    def __init__(self, *args, **kwargs):                                       # trace_info : t_31472
        super().__init__(*args, **kwargs)                                      # trace_info : t_88288, t_155655
        if not self.single_file_per_rank:                                      # trace_info : t_88289, t_155656
            raise NotImplementedError(
                'single_file_per_rank flag not supported for FileSystemWriterAsync'
            )

        # Intermediate state between preparation and finalization
        self.write_buckets: Optional[List[WriteBucket]] = None                 # trace_info : t_88290, t_155657
        self.write_results: Optional[Dict[int, List[WriteResult]]] = None      # trace_info : t_88291, t_155658

    def prepare_write_data(self, plan: SavePlan, planner: SavePlanner) -> None:# trace_info : t_31473
        """
        First stage of async saving. Copy data to CPU and plan the local saving.

        Args:
            plan (SavePlan): save plan generated by the PyT Distributed compatible planner
            planner (SavePlanner): save planner used to resolve the bytes and tensor data

        Returns: None, but stores the save plan in `self.write_buckets`
        """
        storage_plan: _StoragePrefix = plan.storage_data                       # trace_info : t_88329, t_155696
        start = time()                                                         # trace_info : t_88330, t_155697
        logger.debug(f"thread_count: {self.thread_count}, time: {start}")      # trace_info : t_88331, t_155698
        item_buckets = _split_by_size_and_type(self.thread_count, plan.items)  # trace_info : t_88332, t_155699
        logger.debug(f"bucket_prep, time: {time() - start}")                   # trace_info : t_88604, t_155971

        start = time()                                                         # trace_info : t_88605, t_155972
        # move tensors from GPU to CPU before starting async writing
        # We do D2H synchronously for now
        file_count = 0                                                         # trace_info : t_88606, t_155973

        def gen_file():                                                        # trace_info : t_88607, t_155974
            nonlocal file_count
            file_name = f"{storage_plan.prefix}{file_count}{DEFAULT_SUFFIX}"   # trace_info : t_88624, t_88651, t_155991, t_156018
            file_count += 1                                                    # trace_info : t_88625, t_88652, t_155992, t_156019
            return file_name                                                   # trace_info : t_88626, t_88653, t_155993, t_156020

        # Prepare bytes / tensor data in each bucket, which will be assigned to each writer process
        self.write_buckets = []                                                # trace_info : t_88608, t_155975
        for bucket in item_buckets:                                            # trace_info : t_88609, t_88630, t_88657, t_155976, ...
            bytes_data = [                                                     # trace_info : t_88610, t_88612, t_88631, t_88633, t_155977, ...
                (item, planner.resolve_data(item))
                for item in bucket                                             # trace_info : t_88611, t_88632, t_155978, t_155999
                if item.type == WriteItemType.BYTE_IO
            ]
            tensor_data = [                                                    # trace_info : t_88618, t_88620, t_88638, t_88640, t_155985, ...
                (item, planner.resolve_data(item).detach().to("cpu", non_blocking=True))
                for item in bucket                                             # trace_info : t_88619, t_88639, t_155986, t_156006
                if item.type != WriteItemType.BYTE_IO
            ]
            if len(bytes_data) > 0 or len(tensor_data) > 0:                    # trace_info : t_88622, t_88649, t_155989, t_156016
                file_name = gen_file()                                         # trace_info : t_88623, t_88650, t_155990, t_156017
                self.write_buckets.append(                                     # trace_info : t_88627, t_88629, t_88654, t_88656, t_155994, ...
                    (self.path / file_name, file_name, (bytes_data, tensor_data))# trace_info : t_88628, t_88655, t_155995, t_156022
                )

        # Check if there is anything to write on this rank
        if len(self.write_buckets) > 0:                                        # trace_info : t_88658, t_156025
            assert len(self.write_buckets) <= self.thread_count, (             # trace_info : t_88659, t_156026
                len(self.write_buckets),
                self.thread_count,
            )
            ctx = mp.get_context('fork')                                       # trace_info : t_88660, t_156027
            self.write_results = ctx.Manager().dict()                          # trace_info : t_88661, t_156028
        else:
            self.write_results = {}
        logger.debug(f"D2H and push, time: {time() - start}")                  # trace_info : t_88662, t_156029

    def get_save_function_and_args(self) -> Tuple[Optional[Callable], Tuple]:  # trace_info : t_31474
        """
        Get function that saves the data to storage along with its arguments.
        Allows the external caller to apply the save function synchronously or asynchronously.

        Returns: None (if there is nothing to write on this rank) or a tuple of:
            - the function that saves the data
            - arguments to that function
        """
        if not self.write_buckets:                                             # trace_info : t_88668, t_156035
            return None, ()
        return (self.write_preloaded_data_multiproc, (self.write_buckets, self.write_results))# trace_info : t_88669, t_156036

    @staticmethod                                                              # trace_info : t_31475
    def write_preloaded_data_multiproc(                                        # trace_info : t_31476, t_31478, t_31480, t_31482
        write_buckets: List[WriteBucket], write_results: Dict[int, List[WriteResult]]# trace_info : t_31477, t_31479
    ) -> None:                                                                 # trace_info : t_31481
        """
        Performs saving data to storage with multiple processes.

        Args:
            write_buckets (List[WriteBucket]): write plan
            write_results: (Dict[int, List[WriteResult]]): dict to store the write results to.
                Assumes multiprocessing save, so keys are local process indices
        Returns: None
        """
        w_start = time()                                                       # trace_info : t_88677, t_156044
        ctx = mp.get_context('fork')                                           # trace_info : t_88678, t_156045
        p_list = [                                                             # trace_info : t_88679, t_88681, t_156046, t_156048
            ctx.Process(
                target=FileSystemWriterAsync.write_preloaded_data,
                args=(i, write_bucket, write_results, True),
            )
            for i, write_bucket in enumerate(write_buckets)                    # trace_info : t_88680, t_156047
        ]
        for p in p_list:                                                       # trace_info : t_88682, t_88684, t_88686, t_156049, ...
            p.start()                                                          # trace_info : t_88683, t_88685, t_156050, t_156052
        for p in p_list:                                                       # trace_info : t_88687, t_88689, t_88691, t_156054, ...
            p.join()                                                           # trace_info : t_88688, t_88690, t_156055, t_156057

        w_end = time()                                                         # trace_info : t_88692, t_156059
        logger.debug(                                                          # trace_info : t_88693, t_88695, t_156060, t_156062
            f"{w_end}, rank: {torch.distributed.get_rank()}, write(sync,parallel): {w_end - w_start}"# trace_info : t_88694, t_156061
        )

    @staticmethod                                                              # trace_info : t_31483
    def write_preloaded_data(                                                  # trace_info : t_31484, t_31486, t_31488, t_31490, t_31492, ...
        local_proc_idx: int,                                                   # trace_info : t_31485
        write_bucket: WriteBucket,                                             # trace_info : t_31487
        write_results: Dict[int, List[WriteResult]],                           # trace_info : t_31489
        use_fsync: bool,                                                       # trace_info : t_31491
    ) -> None:                                                                 # trace_info : t_31493
        """
        Performs actual data saving to storage.

        Args:
            local_proc_idx (int): index of a local process that performs writing
            write_bucket (WriteBucket): data to write to storage
            write_results (Dict[int, List[WriteResult]]): dict to store the write results to.
                Assumes multiprocessing save, so keys are local process indices
            use_fsync (bool): if True, calls os.fsync at the end of saving

        Returns: None, the write result are written to the `write_results` dict
        """
        mem_before = _process_memory()                                         # trace_info : t_88684, t_88686, t_156051, t_156053

        local_results = []                                                     # trace_info : t_88688, t_88690, t_156055, t_156057
        file_name, storage_key, (bytes_data, tensor_data) = write_bucket       # trace_info : t_88689, t_88691, t_156056, t_156058
        with open(file_name, "wb") as stream:                                  # trace_info : t_88690, t_88692, t_88708, t_88729, t_156057, ...
            for write_item, data in bytes_data:                                # trace_info : t_88691, t_88693, t_88695, t_88697, t_88699, ...
                local_results.append(_write_item(stream, data, write_item, storage_key))# trace_info : t_88692, t_88694, t_88696, t_88698, t_88700, ...

            for write_item, tensor in tensor_data:                             # trace_info : t_88702, t_88702, t_88705, t_88708, t_88711, ...
                assert tensor.is_cpu                                           # trace_info : t_88703, t_88703, t_88706, t_88709, t_88712, ...
                local_results.append(_write_item(stream, tensor, write_item, storage_key))# trace_info : t_88704, t_88704, t_88707, t_88710, t_88713, ...

            if use_fsync:                                                      # trace_info : t_88706, t_88727, t_156073, t_156094
                os.fsync(stream.fileno())                                      # trace_info : t_88707, t_88728, t_156074, t_156095
        write_results[local_proc_idx] = local_results                          # trace_info : t_88709, t_88710, t_88730, t_88731, t_156076, ...
        mem_after = _process_memory()                                          # trace_info : t_88711, t_88732, t_156078, t_156099
        logger.debug(                                                          # trace_info : t_88715, t_88717, t_88736, t_88738, t_156082, ...
            f"{local_proc_idx} consumed: {mem_after - mem_before}, before: {mem_before}, after: {mem_after}"# trace_info : t_88716, t_88737, t_156083, t_156104
        )

    def write_data(self, plan: SavePlan, planner: SavePlanner,) -> Future[List[WriteResult]]:# trace_info : t_31495
        raise NotImplementedError('write_data not implemented for FileSystemWriterAsync')

    def retrieve_write_results(self) -> List[WriteResult]:                     # trace_info : t_31496
        """
        Turn self.write_results into a single results lists. Includes error check.

        Returns (List[WriteResult]): the list of write results from all local processes performing the save.

        """
        assert self.write_results is not None                                  # trace_info : t_88701, t_156068
        assert self.write_buckets is not None                                  # trace_info : t_88702, t_156069
        if len(self.write_results) != len(self.write_buckets):                 # trace_info : t_88703, t_88704, t_156070, t_156071
            raise RuntimeError(
                f'Incomplete worker results (expected {len(self.write_buckets)}, got {len(self.write_results)}.'
                f' This probably indicates a worker failure.'
            )
        return list(chain.from_iterable(self.write_results.values()))          # trace_info : t_88705, t_88706, t_156072, t_156073


def _split_by_size_and_type(bins: int, items: List[WriteItem]) -> List[List[WriteItem]]:# trace_info : t_31497
    """
    Splits write items according to item size into close to uniform bins.

    Same as torch.distributed.checkpoint.filesystem._split_by_size_and_type,
    but with a fixed _item_size function.

    Args:
        bins (int): numbers of bins to split to
        items (List[WriteItem]): list of write items

    Returns (List[List[WriteItem]]): write items split to bins
    """
    if bins == 1:                                                              # trace_info : t_88333, t_155700
        return [items]

    bytes_items = [wi for wi in items if wi.type == WriteItemType.BYTE_IO]     # trace_info : t_88334, t_155701
    tensor_items = [wi for wi in items if wi.type != WriteItemType.BYTE_IO]    # trace_info : t_88335, t_155702

    buckets: List[List[WriteItem]] = [[] for _ in range(bins)]                 # trace_info : t_88336, t_155703
    bucket_sizes = [0 for _ in range(bins)]                                    # trace_info : t_88337, t_155704

    tensor_items.sort(key=_item_size, reverse=True)                            # trace_info : t_88338, t_155705

    # Assign bytes with a simple round-robin
    for i, item in enumerate(bytes_items):                                     # trace_info : t_88434, t_88436, t_88438, t_88440, t_88442, ...
        buckets[i % bins].append(item)                                         # trace_info : t_88435, t_88437, t_88439, t_88441, t_88443, ...

    # Then, assign tensors according to their sizes
    for item in tensor_items:                                                  # trace_info : t_88453, t_88468, t_88485, t_88502, t_88519, ...
        # TODO replace with headq
        idx = min(enumerate(bucket_sizes), key=lambda x: x[1])[0]              # trace_info : t_88454, t_88455, t_88456, t_88469, t_88470, ...
        buckets[idx].append(item)                                              # trace_info : t_88457, t_88472, t_88489, t_88506, t_88523, ...
        bucket_sizes[idx] += _item_size(item)                                  # trace_info : t_88458, t_88473, t_88490, t_88507, t_88524, ...

    return buckets                                                             # trace_info : t_88603, t_155970


def _item_size(item: WriteItem) -> int:                                        # trace_info : t_31498
    """
    Calculates size (in bytes) of a single write item.

    Same as torch.distributed.checkpoint.filesystem._item_size,
    but fixes computing chunk size (with item.tensor_data.chunk.sizes)

    Args:
        item (WriteItem): write item to compute the size of

    Returns (int): size of an item in bytes
    """
    size = 1                                                                   # trace_info : t_88339, t_88350, t_88359, t_88370, t_88381, ...
    assert item.tensor_data is not None                                        # trace_info : t_88340, t_88351, t_88360, t_88371, t_88382, ...
    # can't use math.prod as PT needs to support older python
    for s in item.tensor_data.chunk.sizes:                                     # trace_info : t_88341, t_88343, t_88345, t_88347, t_88352, ...
        size *= s                                                              # trace_info : t_88342, t_88344, t_88346, t_88353, t_88355, ...

    dtype = item.tensor_data.properties.dtype                                  # trace_info : t_88348, t_88357, t_88368, t_88379, t_88390, ...
    return size * torch._utils._element_size(dtype)                            # trace_info : t_88349, t_88358, t_88369, t_88380, t_88391, ...


def _process_memory() -> int:                                                  # trace_info : t_31499
    """
    Get memory used by current process.

    Returns (int): memory used by current process
    """
    process = psutil.Process(os.getpid())                                      # trace_info : t_88685, t_88687, t_88712, t_88733, t_156052, ...
    mem_info = process.memory_info()                                           # trace_info : t_88686, t_88688, t_88713, t_88734, t_156053, ...
    return mem_info.rss                                                        # trace_info : t_88687, t_88689, t_88714, t_88735, t_156054, ...
